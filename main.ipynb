{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13feac6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149876e",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0d0db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Head:\n",
      "   Unnamed: 0        Date       Open       High        Low      Close  \\\n",
      "0           0  2015-01-02  14.858000  14.883333  14.217333  14.620667   \n",
      "1           1  2015-01-05  14.303333  14.433333  13.810667  14.006000   \n",
      "2           2  2015-01-06  14.004000  14.280000  13.614000  14.085333   \n",
      "3           3  2015-01-07  14.223333  14.318667  13.985333  14.063333   \n",
      "4           4  2015-01-08  14.187333  14.253333  14.000667  14.041333   \n",
      "\n",
      "     Volume  \n",
      "0  71466000  \n",
      "1  80527500  \n",
      "2  93928500  \n",
      "3  44526000  \n",
      "4  51637500  \n",
      "Dataset Shape:\n",
      "2274\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df= pd.read_csv('Dataset/Tasla_Stock_Updated_V2.csv')\n",
    "\n",
    "# Check the first few rows to confirm successful import\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"Dataset Shape:\")\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855689b9",
   "metadata": {},
   "source": [
    "### Checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "efbba541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values count: Unnamed: 0    0\n",
      "Date          0\n",
      "Open          0\n",
      "High          0\n",
      "Low           0\n",
      "Close         0\n",
      "Volume        0\n",
      "dtype: int64\n",
      "\n",
      "Total Missing Values: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nMissing Values count: {missing_values}\")\n",
    "\n",
    "total_missing_values = missing_values.sum()\n",
    "print(f\"\\nTotal Missing Values: {total_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f7b87",
   "metadata": {},
   "source": [
    "### Converting date to datetime format and selecting as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9b38fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset after setting Date as index:\n",
      "            Unnamed: 0       Open       High        Low      Close    Volume\n",
      "Date                                                                        \n",
      "2015-01-02           0  14.858000  14.883333  14.217333  14.620667  71466000\n",
      "2015-01-05           1  14.303333  14.433333  13.810667  14.006000  80527500\n",
      "2015-01-06           2  14.004000  14.280000  13.614000  14.085333  93928500\n",
      "2015-01-07           3  14.223333  14.318667  13.985333  14.063333  44526000\n",
      "2015-01-08           4  14.187333  14.253333  14.000667  14.041333  51637500\n"
     ]
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"\\nDataset after setting Date as index:\")\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ba844",
   "metadata": {},
   "source": [
    "### Splitting dataset into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9de3b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2021-01-01'\n",
    "\n",
    "df_train_raw = df[df.index < split_date].copy()\n",
    "df_test_raw = df[df.index >= split_date].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb2582",
   "metadata": {},
   "source": [
    "### Generalized function for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a05b8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df['Monthly_Return'] = df['Close'].pct_change() * 100\n",
    "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['Volatility_5'] = df['Close'].rolling(window=5).std()\n",
    "    df['Volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "    df['Volatility_20'] = df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    df.dropna(inplace=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7919b4b",
   "metadata": {},
   "source": [
    "### Creating features for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3be8eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1492, 13)\n",
      "Test shape: (744, 13)\n",
      "Train head:\n",
      "             Unnamed: 0       Open       High        Low      Close    Volume  \\\n",
      "Date                                                                           \n",
      "2015-01-30          19  13.597333  13.831333  13.533333  13.573333  45105000   \n",
      "2015-02-02          20  13.598000  14.130000  13.553333  14.062667  62238000   \n",
      "2015-02-03          21  14.214667  14.691333  14.084667  14.557333  72393000   \n",
      "2015-02-04          22  14.552667  14.765333  14.453333  14.570000  49581000   \n",
      "2015-02-05          23  14.658667  15.032000  14.642667  14.732667  52843500   \n",
      "\n",
      "            Monthly_Return        MA5       MA10       MA20  Volatility_5  \\\n",
      "Date                                                                        \n",
      "2015-01-30       -0.779733  13.609333  13.367867  13.550367      0.192533   \n",
      "2015-02-02        3.605114  13.667867  13.487000  13.522467      0.278766   \n",
      "2015-02-03        3.517584  13.832933  13.663200  13.550033      0.490317   \n",
      "2015-02-04        0.087013  14.088667  13.809733  13.574267      0.470261   \n",
      "2015-02-05        1.116454  14.299200  13.938867  13.607733      0.477150   \n",
      "\n",
      "            Volatility_10  Volatility_20  \n",
      "Date                                      \n",
      "2015-01-30       0.348244       0.497418  \n",
      "2015-02-02       0.362972       0.447356  \n",
      "2015-02-03       0.413990       0.493343  \n",
      "2015-02-04       0.451921       0.531453  \n",
      "2015-02-05       0.515043       0.582495  \n",
      "Test head:\n",
      "             Unnamed: 0        Open        High         Low       Close  \\\n",
      "Date                                                                     \n",
      "2021-02-01        1530  271.429993  280.666656  265.186676  279.936676   \n",
      "2021-02-02        1531  281.559998  293.500000  280.733337  290.929993   \n",
      "2021-02-03        1532  292.339996  292.693329  284.353333  284.896667   \n",
      "2021-02-04        1533  285.000000  285.500000  277.806671  283.329987   \n",
      "2021-02-05        1534  281.666656  288.256653  279.656677  284.076660   \n",
      "\n",
      "              Volume  Monthly_Return         MA5        MA10        MA20  \\\n",
      "Date                                                                       \n",
      "2021-02-01  76174200        5.832167  281.068005  282.781668  276.442834   \n",
      "2021-02-02  73038600        3.927073  280.381335  283.723001  278.826500   \n",
      "2021-02-03  55030500       -2.073807  279.750000  283.864334  280.819500   \n",
      "2021-02-04  47438100       -0.549912  280.720667  284.031000  282.386333   \n",
      "2021-02-05  55699800        0.263535  284.633997  284.217334  282.989499   \n",
      "\n",
      "            Volatility_5  Volatility_10  Volatility_20  \n",
      "Date                                                    \n",
      "2021-02-01     11.266357       8.453199      14.894892  \n",
      "2021-02-02     10.318169       8.813140      12.998415  \n",
      "2021-02-03      9.815727       8.820200      10.325927  \n",
      "2021-02-04      9.897953       8.789682       7.787053  \n",
      "2021-02-05      3.993258       8.766588       7.398818  \n"
     ]
    }
   ],
   "source": [
    "df_train = create_features(df_train_raw)\n",
    "df_test = create_features(df_test_raw)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "print(\"Train head:\\n\", df_train.head())\n",
    "print(\"Test head:\\n\", df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "730d7d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Train shape: (1492, 13)\n",
      "Scaled Test shape: (744, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_features_minmax(df_train, df_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Applies MinMax scaling to the given feature columns.\n",
    "    Scaler is fit on train data only to prevent data leakage.\n",
    "\n",
    "    Parameters:\n",
    "        df_train (pd.DataFrame): Training dataset\n",
    "        df_test (pd.DataFrame): Testing dataset\n",
    "        feature_cols (list): List of column names to scale\n",
    "\n",
    "    Returns:\n",
    "        df_train_scaled (pd.DataFrame): Scaled training dataset\n",
    "        df_test_scaled (pd.DataFrame): Scaled testing dataset\n",
    "        scaler (MinMaxScaler): The fitted scaler object\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Copy to preserve original data\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_test_scaled = df_test.copy()\n",
    "\n",
    "    # Fit scaler on training data, transform both\n",
    "    df_train_scaled[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n",
    "    df_test_scaled[feature_cols] = scaler.transform(df_test[feature_cols]) #only transoform to avoid data leakage\n",
    "\n",
    "    return df_train_scaled, df_test_scaled, scaler\n",
    "\n",
    "\n",
    "# Define the feature columns to scale\n",
    "features_to_scale = ['Close', 'Monthly_Return', 'MA5', 'MA10', 'MA20',\n",
    "                     'Volatility_5', 'Volatility_10', 'Volatility_20']\n",
    "\n",
    "# Call the function\n",
    "df_train_scaled, df_test_scaled, fitted_scaler = scale_features_minmax(df_train, df_test, features_to_scale)\n",
    "print(\"Scaled Train shape:\", df_train_scaled.shape)\n",
    "print(\"Scaled Test shape:\", df_test_scaled.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
