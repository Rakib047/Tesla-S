{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3510fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from Utils.Data_Preprocessing import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load both .pkl and .h5 models from the Models directory.\n",
    "    Simply loads and prints model types without doing any inference.\n",
    "    \"\"\"\n",
    "    # Define path to your Models directory\n",
    "    project_root = Path('.')  # Current directory\n",
    "    models_dir = project_root / 'Models'\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not models_dir.exists():\n",
    "        print(f\"Error: Models directory not found at {models_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading models from: {models_dir}\")\n",
    "    \n",
    "    # Define model files based on your directory structure\n",
    "    pkl_models = [\n",
    "        'decision_tree_model.pkl',\n",
    "        'linear_regression_model.pkl',\n",
    "        'prophet_model.pkl',\n",
    "        'random_forest_model.pkl',\n",
    "        'slinear_regression_model.pkl',\n",
    "        'svr_model.pkl',\n",
    "        'voting_model.pkl',\n",
    "        'xgboost_model.pkl'\n",
    "    ]\n",
    "    \n",
    "    h5_models = [\n",
    "        'gru_model.h5',\n",
    "        'lstm_model.h5'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary to store loaded models\n",
    "    loaded_models = {}\n",
    "    \n",
    "    # Load pickle models\n",
    "    print(\"\\n--- Loading Pickle Models ---\")\n",
    "    for model_file in pkl_models:\n",
    "        model_path = models_dir / model_file\n",
    "        try:\n",
    "            # Only try to load if file exists\n",
    "            if model_path.exists():\n",
    "                model = joblib.load(model_path)\n",
    "                model_name = model_path.stem  # Get filename without extension\n",
    "                loaded_models[model_name] = model\n",
    "                \n",
    "                # Check model type\n",
    "                model_type = type(model).__name__\n",
    "                has_predict = hasattr(model, 'predict')\n",
    "                \n",
    "                print(f\"✓ {model_name}: Type={model_type}, Has predict method={has_predict}\")\n",
    "            else:\n",
    "                print(f\"⚠ {model_file} not found, skipping\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {model_file}: {e}\")\n",
    "    \n",
    "    # Load h5 models (TensorFlow/Keras models)\n",
    "    print(\"\\n--- Loading H5 Models ---\")\n",
    "    for model_file in h5_models:\n",
    "        model_path = models_dir / model_file\n",
    "        try:\n",
    "            # Only try to load if file exists\n",
    "            if model_path.exists():\n",
    "                # TensorFlow requires string path\n",
    "                model = tf.keras.models.load_model(str(model_path))\n",
    "                model_name = model_path.stem  # Get filename without extension\n",
    "                loaded_models[model_name] = model\n",
    "                \n",
    "                # Check model type and summary\n",
    "                model_type = type(model).__name__\n",
    "                has_predict = hasattr(model, 'predict')\n",
    "                \n",
    "                print(f\"✓ {model_name}: Type={model_type}, Has predict method={has_predict}\")\n",
    "                \n",
    "                # Print model summary\n",
    "                print(f\"Model structure:\")\n",
    "                #model.summary()\n",
    "            else:\n",
    "                print(f\"⚠ {model_file} not found, skipping\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {model_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(loaded_models)} models\")\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d32a4913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from: Models\n",
      "\n",
      "--- Loading Pickle Models ---\n",
      "✓ decision_tree_model: Type=DecisionTreeRegressor, Has predict method=True\n",
      "✓ linear_regression_model: Type=LinearRegression, Has predict method=True\n",
      "✓ prophet_model: Type=Prophet, Has predict method=True\n",
      "✓ random_forest_model: Type=RandomForestRegressor, Has predict method=True\n",
      "⚠ slinear_regression_model.pkl not found, skipping\n",
      "✓ svr_model: Type=SVR, Has predict method=True\n",
      "✓ voting_model: Type=VotingRegressor, Has predict method=True\n",
      "✓ xgboost_model: Type=XGBRegressor, Has predict method=True\n",
      "\n",
      "--- Loading H5 Models ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ gru_model: Type=Sequential, Has predict method=True\n",
      "Model structure:\n",
      "✓ lstm_model: Type=Sequential, Has predict method=True\n",
      "Model structure:\n",
      "\n",
      "Successfully loaded 9 models\n",
      "✓ decision_tree_model prediction: 20.1507\n",
      "✓ linear_regression_model prediction: 23.3113\n",
      "✗ Error predicting with prophet_model: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "✓ random_forest_model prediction: 22.9479\n",
      "✓ svr_model prediction: 46.4437\n",
      "✓ voting_model prediction: 28.9141\n",
      "✓ xgboost_model prediction: 22.9535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision_tree_model': 20.15066719055176,\n",
       " 'linear_regression_model': 23.311288549092335,\n",
       " 'random_forest_model': 22.947899913787843,\n",
       " 'svr_model': 46.44371812038351,\n",
       " 'voting_model': 28.914106112508616,\n",
       " 'xgboost_model': 22.953518}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference_classical_models(data):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "\n",
    "    # Load the scalers\n",
    "    scaler_X = joblib.load('Scaler/scaler_X_minmax.pkl')\n",
    "    scaler_y = joblib.load('Scaler/scaler_y_minmax.pkl')\n",
    "    #model = joblib.load('Models/linear_regression_model.pkl')\n",
    "    \n",
    "    loaded_models= load_models()\n",
    "    \n",
    "    # Filter only classical models (skip keras models which are of type 'Functional', 'Sequential', etc.)\n",
    "    classical_models = {\n",
    "        name: model for name, model in loaded_models.items()\n",
    "        if not isinstance(model, tf.keras.Model)\n",
    "    }\n",
    "\n",
    "    # # Read last 20 rows (not 19!) for moving average support\n",
    "    # df_hist = pd.read_csv('Data/Tasla_Stock_Updated_V2.csv').tail(20)\n",
    "\n",
    "    # # Add the new data (must match expected column names)\n",
    "    # df_new = pd.DataFrame([data], columns=['Low', 'High', 'Open', 'Close', 'Volume'])\n",
    "    # df_combined = pd.concat([df_hist, df_new], ignore_index=True)\n",
    "    \n",
    "    # # Select features for prediction (same as used during training)\n",
    "    # features = ['Low', 'High', 'Open', 'Close', 'Volume']\n",
    "    # # Extract the last row (new data) with features\n",
    "    # X = df_combined[features].iloc[-1:].values  # Shape: (1, n_features)\n",
    "\n",
    "    # Scale the features\n",
    "    X_scaled = scaler_X.transform(np.array(data).reshape(1, -1))\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    # Inference using each classical model\n",
    "    for name, model in classical_models.items():\n",
    "        try:\n",
    "            y_pred_scaled = model.predict(X_scaled)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "            predictions[name] = y_pred[0][0]\n",
    "            print(f\"✓ {name} prediction: {y_pred[0][0]:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error predicting with {name}: {e}\")\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "\n",
    "inference_classical_models([23.543333053588867,23.83133316040039,22.858667373657227,22.99933242797852,114088500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
